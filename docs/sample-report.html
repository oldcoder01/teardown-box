<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Teardown Report (Sample)</title>
  <style>
    :root {
      --bg: #ffffff;
      --fg: #111827;
      --muted: #6b7280;
      --border: #e5e7eb;
      --codebg: #f6f8fa;
      --link: #2563eb;
    }
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
      margin: 0;
      background: var(--bg);
      color: var(--fg);
      line-height: 1.55;
    }
    .wrap {
      max-width: 1000px;
      margin: 0 auto;
      padding: 2rem 1rem;
    }
    h1, h2, h3, h4 {
      line-height: 1.25;
      margin-top: 1.6em;
    }
    h1 { margin-top: 0; }
    a { color: var(--link); text-decoration: none; }
    a:hover { text-decoration: underline; }
    code, pre {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    }
    pre {
      background: var(--codebg);
      padding: 0.9rem;
      border-radius: 10px;
      overflow-x: auto;
      border: 1px solid var(--border);
    }
    code {
      background: var(--codebg);
      padding: 0.15rem 0.3rem;
      border-radius: 6px;
      border: 1px solid var(--border);
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
      border: 1px solid var(--border);
      border-radius: 12px;
      overflow: hidden;
      display: block;
    }
    th, td {
      padding: 0.55rem 0.6rem;
      border-bottom: 1px solid var(--border);
      vertical-align: top;
    }
    th {
      text-align: left;
      background: #f9fafb;
      font-weight: 600;
    }
    tr:last-child td { border-bottom: none; }
    blockquote {
      margin: 1rem 0;
      padding: 0.75rem 1rem;
      border-left: 4px solid var(--border);
      background: #fafafa;
      color: var(--muted);
    }
    .toc {
      padding: 1rem;
      border: 1px solid var(--border);
      border-radius: 12px;
      background: #fcfcfc;
    }
    details {
      margin: 0.4rem 0 1.0rem;
      padding: 0.65rem 0.8rem;
      border: 1px solid var(--border);
      border-radius: 12px;
      background: #fcfcfc;
    }
    summary {
      cursor: pointer;
      font-weight: 600;
    }
    hr {
      border: none;
      border-top: 1px solid var(--border);
      margin: 1.8rem 0;
    }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="toc">
<ul>
<li><a href="#teardown-report-sample">Teardown Report (Sample)</a><ul>
<li><a href="#teardown-in-a-box-48-hour-non-invasive-teardown">Teardown in a Box (48-hour non-invasive teardown)</a></li>
<li><a href="#executive-summary">Executive summary</a></li>
<li><a href="#inputs-reviewed-scope">Inputs reviewed (scope)</a></li>
<li><a href="#top-3-fix-now-wins-highest-roi">Top 3 fix-now wins (highest ROI)</a></li>
<li><a href="#triage-table-skim-friendly">Triage table (skim-friendly)</a></li>
<li><a href="#assumptions-limits-what-this-report-is-isnt">Assumptions &amp; limits (what this report is / isn't)</a></li>
<li><a href="#scoring-rubric-how-to-read-this-report">Scoring rubric (how to read this report)</a></li>
<li><a href="#what-i-need-from-you-access-options">What I need from you (access options)</a></li>
<li><a href="#what-i-would-verify-with-real-access-48-hour-verification-plan">What I would verify with real access (48-hour verification plan)</a></li>
<li><a href="#findings">Findings</a><ul>
<li><a href="#security">Security</a><ul>
<li><a href="#high-unexpected-public-listener-detected-on-port-5432">[High] Unexpected public listener detected on port 5432</a></li>
<li><a href="#medium-legacy-tls-versions-appear-enabled-tls-1011">[Medium] Legacy TLS versions appear enabled (TLS 1.0/1.1)</a></li>
<li><a href="#low-hsts-is-missing">[Low] HSTS is missing</a></li>
</ul>
</li>
<li><a href="#reliability">Reliability</a><ul>
<li><a href="#high-connection-pool-appears-saturated-high-client-usage-waiting-queue">[High] Connection pool appears saturated (high client usage / waiting queue)</a></li>
<li><a href="#high-systemd-service-appears-to-be-flapping-restart-loop">[High] systemd service appears to be flapping (restart loop)</a></li>
<li><a href="#medium-autovacuum-pressure-likely-on-publicorders-publicevents-publicsessions-with-high-dead-tuple-ratios">[Medium] Autovacuum pressure likely on (public.orders, public.events, public.sessions) with high dead tuple ratios</a></li>
<li><a href="#medium-disk-usage-is-high-87-on-at-least-one-filesystem">[Medium] Disk usage is high (87%) on at least one filesystem</a></li>
</ul>
</li>
<li><a href="#performance">Performance</a><ul>
<li><a href="#high-high-sequential-scan-activity-on-large-tables-publicorders-publicevents">[High] High sequential scan activity on large tables (public.orders, public.events)</a></li>
<li><a href="#high-postgres-shows-heavy-time-spent-in-a-small-set-of-queries-pg_stat_statements">[High] Postgres shows heavy time spent in a small set of queries (pg_stat_statements)</a></li>
</ul>
</li>
<li><a href="#cost">Cost</a><ul>
<li><a href="#medium-possible-overprovisioning-signal-m52xlarge-at-low-p95-utilization">[Medium] Possible overprovisioning signal: m5.2xlarge at low p95 utilization</a></li>
<li><a href="#low-ebs-gp2-volumes-detected-consider-gp3-for-costperformance-control">[Low] EBS gp2 volumes detected; consider gp3 for cost/performance control</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#raw-evidence">Raw evidence</a><ul>
<li><a href="#fixturespostgrespg_stat_user_tablescsv-tables-with-high-n_dead_tup-relative-to-n_live_tup">fixtures/postgres/pg_stat_user_tables.csv (Tables with high n_dead_tup relative to n_live_tup)</a></li>
<li><a href="#fixturesedgetls_scantxt-hsts-marked-missing-in-scan-summary">fixtures/edge/tls_scan.txt (HSTS marked missing in scan summary)</a></li>
<li><a href="#fixtureslinuxss_lntptxtl5-l5-bound-to-00005432">fixtures/linux/ss_lntp.txt:L5-L5 (Bound to 0.0.0.0:5432)</a></li>
<li><a href="#fixturespostgrespg_stat_statementscsvl1-l4-top-queries-by-total_time_ms-sample">fixtures/postgres/pg_stat_statements.csv:L1-L4 (Top queries by total_time_ms (sample))</a></li>
<li><a href="#fixturescostebs_volumescsv-gp2-volumes-vol-0aaa111">fixtures/cost/ebs_volumes.csv (gp2 volumes: vol-0aaa111)</a></li>
<li><a href="#fixturespostgrespg_pool_statsjson-clients48605000-waiting322-avg_wait_ms1850">fixtures/postgres/pg_pool_stats.json (clients=4860/5000, waiting=322, avg_wait_ms=185.0)</a></li>
<li><a href="#fixturescostutilization_summaryjson-instancei-0123456789abcdef0-cpu_p95124-mem_p95281-over-30-days">fixtures/cost/utilization_summary.json (instance=i-0123456789abcdef0, cpu_p95=12.4%, mem_p95=28.1% over 30 days)</a></li>
<li><a href="#fixturesedgetls_scantxt-tlsv10tlsv11-enabled-in-scan-summary">fixtures/edge/tls_scan.txt (TLSv1.0/TLSv1.1 enabled in scan summary)</a></li>
<li><a href="#fixturespostgrespg_stat_user_tablescsv-tables-with-high-reltuples-and-high-seq_scan">fixtures/postgres/pg_stat_user_tables.csv (Tables with high reltuples and high seq_scan)</a></li>
<li><a href="#fixtureslinuxdf_htxtl2-l2-filesystem-above-threshold-devnvme0n1p2-100g-87g-13g-87">fixtures/linux/df_h.txt:L2-L2 (Filesystem above threshold: /dev/nvme0n1p2 100G 87G 13G 87% /)</a></li>
<li><a href="#fixtureslinuxsystemctl_statustxtl8-l8-restart-counter-is-27">fixtures/linux/systemctl_status.txt:L8-L8 (Restart counter is 27)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h1 id="teardown-report-sample">Teardown Report (Sample)</h1>
<h2 id="teardown-in-a-box-48-hour-non-invasive-teardown">Teardown in a Box (48-hour non-invasive teardown)</h2>
<p><strong>What this is:</strong> A fast, non-invasive teardown that turns a messy system into a prioritized fix list.
<strong>What you get:</strong> A report like this + a short call to confirm priorities + a fix-now shortlist.
<strong>Who it's for:</strong> Small SaaS / agencies / teams with recurring incidents, slow Postgres, and unclear next steps.</p>
<p><strong>Next step:</strong> <a href="https://calendly.com/jason-kelly-001/introcall">Book 15 minutes</a> — info@buzzyplanet.com</p>
<p><em>Generated: 2026-01-05T12:56:44-08:00</em></p>
<h2 id="executive-summary">Executive summary</h2>
<ul>
<li>Findings: 11 total</li>
<li>High: 5</li>
<li>Medium: 4</li>
<li>Low: 2</li>
</ul>
<h2 id="inputs-reviewed-scope">Inputs reviewed (scope)</h2>
<p>This report is generated from the following snapshot artifacts (synthetic fixtures in this demo).</p>
<ul>
<li><code>cost/ebs_volumes.csv</code></li>
<li><code>cost/utilization_summary.json</code></li>
<li><code>edge/nginx.conf</code></li>
<li><code>edge/tls_scan.txt</code></li>
<li><code>infra/terraform_plan.txt</code></li>
<li><code>linux/df_h.txt</code></li>
<li><code>linux/ss_lntp.txt</code></li>
<li><code>linux/systemctl_status.txt</code></li>
<li><code>postgres/pg_pool_stats.json</code></li>
<li><code>postgres/pg_stat_statements.csv</code></li>
<li><code>postgres/pg_stat_user_tables.csv</code></li>
</ul>
<h2 id="top-3-fix-now-wins-highest-roi">Top 3 fix-now wins (highest ROI)</h2>
<ul>
<li><strong>[High] High sequential scan activity on large tables (public.orders, public.events)</strong> (Effort: Medium, Blast radius: Medium) — Fix now: <em>Identify query patterns causing seq_scans and add targeted indexes</em></li>
<li><strong>[High] Postgres shows heavy time spent in a small set of queries (pg_stat_statements)</strong> (Effort: Medium, Blast radius: Medium) — Fix now: <em>Validate query plans and implement the highest-impact index/query changes</em></li>
<li><strong>[High] Connection pool appears saturated (high client usage / waiting queue)</strong> (Effort: Medium, Blast radius: Medium) — Fix now: <em>Reduce pool pressure and protect the DB from connection storms</em></li>
</ul>
<h2 id="triage-table-skim-friendly">Triage table (skim-friendly)</h2>
<table>
<thead>
<tr>
<th>Sev</th>
<th>Area</th>
<th>Finding</th>
<th>Why it matters</th>
<th>Fix-now</th>
<th>Effort</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr>
<td>Medium</td>
<td>Cost</td>
<td><a href="#finding-0001"><strong>Possible overprovisioning signal: m5.2xlarge at low p95 utilization</strong></a></td>
<td>If sustained utilization is low, you may be paying for capacity you don't need</td>
<td>Create a rightsizing candidate and validate against peak/burst patterns</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td>Low</td>
<td>Cost</td>
<td><a href="#finding-0002"><strong>EBS gp2 volumes detected; consider gp3 for cost/performance control</strong></a></td>
<td>gp3 often provides better baseline performance and more predictable tuning</td>
<td>Evaluate gp3 migration plan (low-risk, validate per workload)</td>
<td>Low</td>
<td>Medium</td>
</tr>
<tr>
<td>High</td>
<td>Performance</td>
<td><a href="#finding-0003"><strong>High sequential scan activity on large tables (public.orders, public.events)</strong></a></td>
<td>Repeated sequential scans on large tables inflate latency and CPU, especially under concurrency</td>
<td>Identify query patterns causing seq_scans and add targeted indexes</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>High</td>
<td>Performance</td>
<td><a href="#finding-0004"><strong>Postgres shows heavy time spent in a small set of queries (pg_stat_statements)</strong></a></td>
<td>A handful of queries often dominate database load</td>
<td>Validate query plans and implement the highest-impact index/query changes</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>High</td>
<td>Reliability</td>
<td><a href="#finding-0005"><strong>Connection pool appears saturated (high client usage / waiting queue)</strong></a></td>
<td>When the pool saturates, requests queue and tail latency spikes</td>
<td>Reduce pool pressure and protect the DB from connection storms</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>High</td>
<td>Reliability</td>
<td><a href="#finding-0006"><strong>systemd service appears to be flapping (restart loop)</strong></a></td>
<td>Restart loops create intermittent downtime, amplify load (retry storms), and usually mask a real dependency issue (DB, DNS, config, or secr…</td>
<td>Pull recent logs and verify dependencies; add backoff while fixing root cause</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>Medium</td>
<td>Reliability</td>
<td><a href="#finding-0007"><strong>Autovacuum pressure likely on (public.orders, public.events, public.sessions) with high dead tuple ratios</strong></a></td>
<td>High dead tuples increase bloat and slow queries (more pages to scan, worse cache locality)</td>
<td>Inspect worst tables and tune vacuum/analyze thresholds where needed</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>Medium</td>
<td>Reliability</td>
<td><a href="#finding-0008"><strong>Disk usage is high (87%) on at least one filesystem</strong></a></td>
<td>High disk usage is a common outage trigger (writes fail, services crash, databases stall)</td>
<td>Identify top disk consumers and cap runaway logs safely</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>High</td>
<td>Security</td>
<td><a href="#finding-0009"><strong>Unexpected public listener detected on port 5432</strong></a></td>
<td>Public listeners expand the attack surface</td>
<td>Restrict bind address and enforce network controls</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>Medium</td>
<td>Security</td>
<td><a href="#finding-0010"><strong>Legacy TLS versions appear enabled (TLS 1.0/1.1)</strong></a></td>
<td>Older TLS versions weaken security posture and may violate compliance expectations</td>
<td>Disable TLS 1.0/1.1 and standardize a modern policy</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>Low</td>
<td>Security</td>
<td><a href="#finding-0011"><strong>HSTS is missing</strong></a></td>
<td>Without HSTS, clients can be tricked into initial HTTP connections in some downgrade scenarios</td>
<td>Add an HSTS header after validating HTTPS-only readiness</td>
<td>Medium</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<h2 id="assumptions-limits-what-this-report-is-isnt">Assumptions &amp; limits (what this report is / isn't)</h2>
<p>This report is based on a point-in-time snapshot of the artifacts listed in <strong>Inputs reviewed</strong>.
It intentionally avoids invasive actions. Findings are prioritized by <em>severity x impact x confidence</em>.</p>
<p><strong>Assumptions</strong>
- The snapshot is representative of normal and peak behavior (or at least of a recent incident window).
- The environment is a typical internet-facing web app with a Postgres-backed data tier.
- "Fix now" commands are intended to be safe and reversible, but should be validated in your environment.</p>
<p><strong>Limits</strong>
- No live access was used for this demo; real environments require verification (config drift, active traffic, dependencies).
- Cost signals are directional (rightsizing recommendations require longer windows and peak analysis).
- Some findings depend on business context (SLOs, traffic patterns, compliance requirements).</p>
<h2 id="scoring-rubric-how-to-read-this-report">Scoring rubric (how to read this report)</h2>
<p><strong>Severity</strong>
- <strong>Critical</strong>: active or likely outage/security exposure; fix immediately.
- <strong>High</strong>: probable incident, meaningful security risk, or major user impact.
- <strong>Medium</strong>: important hygiene/perf work; schedule into the next sprint.
- <strong>Low</strong>: improvements/opportunities; do when convenient or bundle with other work.</p>
<p><strong>Confidence</strong>
- <strong>High</strong>: direct evidence in artifacts (clear config/log/metric signal).
- <strong>Medium</strong>: strong indicators, but needs a quick confirm in live systems.
- <strong>Low</strong>: directional signal; must validate before acting (common for cost).</p>
<p><strong>Effort</strong>
- <strong>Low</strong>: &lt; 1–2 hours to ship a safe change (plus verification).
- <strong>Medium</strong>: 0.5–2 days, coordination or testing required.
- <strong>High</strong>: multi-day work, refactors, migrations, or multiple systems touched.</p>
<p><strong>Blast radius</strong>
- <strong>Low</strong>: isolated change; easy rollback.
- <strong>Medium</strong>: touches shared components (LB/DB/pooler); requires coordination.
- <strong>High</strong>: broad change surface; run a staged rollout + explicit rollback plan.</p>
<h2 id="what-i-need-from-you-access-options">What I need from you (access options)</h2>
<p>Prospects often worry that a contractor needs deep access. This teardown can be done in tiers:</p>
<p><strong>Option A — No access (artifact-only)</strong>
- You send a snapshot bundle (similar to the fixtures in this demo): service status, configs, key metrics exports, database stats.
- Best for fast triage and a prioritized plan.</p>
<p><strong>Option B — Read-only access (preferred for accuracy)</strong>
- Cloud metrics (CloudWatch/Stackdriver), load balancer stats, DB performance insights, logs (read-only), and IaC state/plan output.
- This turns "likely" into "we're sure" without invasive changes.</p>
<p><strong>Option C — Timeboxed elevated access (rare)</strong>
- Only if required for a specific fix (e.g., emergency mitigation), with explicit scope and a rollback plan.</p>
<h2 id="what-i-would-verify-with-real-access-48-hour-verification-plan">What I would verify with real access (48-hour verification plan)</h2>
<p>This is the short list of checks I run to turn snapshot findings into "we're sure" conclusions.</p>
<p><strong>Security</strong>
- Confirm firewall/SG reality vs host listeners (what is actually reachable from the internet).
- Validate TLS termination point (CDN/WAF vs origin) and confirm enforced TLS versions/ciphers.
- Review IAM/service principals for least-privilege (read/write boundaries, credential rotation).</p>
<p><strong>Reliability</strong>
- Confirm restart-loop root cause via logs + dependency checks (DB reachability, DNS, secrets/config).
- Validate disk growth source (log volume, retention policy, and whether growth is correlated to incidents/deploys).
- Verify alerting: paging thresholds, on-call policy, and whether alerts are actionable.</p>
<p><strong>Performance</strong>
- Run EXPLAIN (ANALYZE, BUFFERS) on top queries with representative parameters.
- Identify whether seq scans are driven by hot endpoints, background jobs, or analytics queries.
- Check lock contention and connection churn (pg_stat_activity, pooler stats, deploy windows).</p>
<p><strong>Cost</strong>
- Pull 30–90 days utilization and include peak events; validate headroom requirements.
- Confirm storage policy: gp2/gp3 usage, snapshot retention, orphaned volumes.
- Tie spend to workload (unit cost per request/job) to avoid "savings that break SLOs".</p>
<p><strong>Exit criteria (what "done" looks like)</strong>
- Top risks have owners, rollback plans, and a validated implementation path.
- We agree on SLOs (or at least p95/p99 targets) to define "improvement."
- A 7-day stabilization plan + 30-day hardening plan is accepted by the team.</p>
<h2 id="findings">Findings</h2>
<h3 id="security">Security</h3>
<p><a id="finding-0009"></a></p>
<h4 id="high-unexpected-public-listener-detected-on-port-5432">[High] Unexpected public listener detected on port 5432</h4>
<p><strong>Impact:</strong> Public listeners expand the attack surface. Databases and caches should not be exposed to the internet without strong justification, network controls, and monitoring.</p>
<p><strong>Confidence:</strong> High</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-38959d52f8">fixtures/linux/ss_lntp.txt:L5-L5 (Bound to 0.0.0.0:5432)</a></p>
<p><strong>Fix now:</strong> Restrict bind address and enforce network controls</p>
<pre><code class="language-bash"># If this is Postgres, prefer listen_addresses='localhost' (or private subnet only)
# Verify cloud SG/firewall: deny inbound 5432 from 0.0.0.0/0
sudo ufw status || true
sudo ss -lntp | head -50
</code></pre>
<p><strong>7-day plan:</strong>
- Confirm which services should be public and document an explicit allowlist.
- Restrict binds to localhost/private interfaces and enforce SG/firewall rules.
- Add monitoring/alerting for new public listeners.</p>
<p><strong>30-day plan:</strong>
- Standardize hardening baselines (CIS-ish) for hosts and containers.
- Add continuous drift detection (ports, firewall, SGs) as a scheduled check.
- Adopt least-privilege network segmentation between app and data tiers.</p>
<p><strong>Questions I need answered:</strong>
- Is port 5432 intentionally public (e.g., temporary debug, migration)?
- What enforces network policy today (security groups, nftables, kubernetes, etc.)?
- Do you have a documented threat model / compliance constraints?</p>
<p><a id="finding-0010"></a></p>
<h4 id="medium-legacy-tls-versions-appear-enabled-tls-1011">[Medium] Legacy TLS versions appear enabled (TLS 1.0/1.1)</h4>
<p><strong>Impact:</strong> Older TLS versions weaken security posture and may violate compliance expectations. Most modern clients support TLS 1.2+.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-8eba99682e">fixtures/edge/tls_scan.txt (TLSv1.0/TLSv1.1 enabled in scan summary)</a></p>
<p><strong>Fix now:</strong> Disable TLS 1.0/1.1 and standardize a modern policy</p>
<pre><code class="language-bash"># Target: TLS 1.2 and 1.3 only (exact config depends on your edge stack).
ssl_protocols TLSv1.2 TLSv1.3;
# Use a modern cipher suite policy appropriate to your environment.
</code></pre>
<p><strong>7-day plan:</strong>
- Confirm client compatibility requirements (legacy devices/browsers).
- Disable TLS 1.0/1.1 and redeploy edge config.
- Run a follow-up scan to confirm posture.</p>
<p><strong>30-day plan:</strong>
- Automate TLS posture scans (scheduled) and alert on regressions.
- Adopt managed TLS policies via CDN/WAF where feasible.
- Track certificate renewal and config drift.</p>
<p><strong>Questions I need answered:</strong>
- Do you terminate TLS at a CDN/WAF or on the origin?
- Any compliance requirements (PCI/HIPAA/SOC2) driving a specific policy?
- Any legacy clients that truly require TLS 1.0/1.1?</p>
<p><a id="finding-0011"></a></p>
<h4 id="low-hsts-is-missing">[Low] HSTS is missing</h4>
<p><strong>Impact:</strong> Without HSTS, clients can be tricked into initial HTTP connections in some downgrade scenarios. HSTS is usually a low-risk hardening win for public HTTPS sites.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-23fd6b3517">fixtures/edge/tls_scan.txt (HSTS marked missing in scan summary)</a></p>
<p><strong>Fix now:</strong> Add an HSTS header after validating HTTPS-only readiness</p>
<pre><code class="language-bash">add_header Strict-Transport-Security &quot;max-age=31536000; includeSubDomains&quot; always;
# Consider preload only after careful validation.
</code></pre>
<p><strong>7-day plan:</strong>
- Confirm all subdomains are HTTPS and redirects are correct.
- Deploy HSTS and validate no mixed-content regressions.</p>
<p><strong>30-day plan:</strong>
- Add a baseline set of security headers (CSP, X-Content-Type-Options, etc.).
- Automate header checks in CI.</p>
<p><strong>Questions I need answered:</strong>
- Are there any HTTP-only subdomains/endpoints still in use?
- Do you already use a CDN/WAF that can set headers globally?</p>
<h3 id="reliability">Reliability</h3>
<p><a id="finding-0005"></a></p>
<h4 id="high-connection-pool-appears-saturated-high-client-usage-waiting-queue">[High] Connection pool appears saturated (high client usage / waiting queue)</h4>
<p><strong>Impact:</strong> When the pool saturates, requests queue and tail latency spikes. This often presents as timeouts and cascading retries, which further increases load.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-5ee8609c2c">fixtures/postgres/pg_pool_stats.json (clients=4860/5000, waiting=322, avg_wait_ms=185.0)</a></p>
<p><strong>Fix now:</strong> Reduce pool pressure and protect the DB from connection storms</p>
<pre><code class="language-bash"># Align app pool sizes to DB capacity and reduce per-instance pools if needed.
# Consider transaction pooling for short-lived queries (if compatible).
psql -c &quot;SHOW max_connections;&quot;
psql -c &quot;SELECT state, count(*) FROM pg_stat_activity GROUP BY state;&quot;
</code></pre>
<p><strong>7-day plan:</strong>
- Inventory all services connecting to Postgres and their pool sizes.
- Set sane timeouts/backoff to prevent retry storms when DB is slow.
- Correlate pool wait spikes with deploy windows, traffic, and slow queries.</p>
<p><strong>30-day plan:</strong>
- Introduce admission control (rate limiting/load shedding) for hot endpoints.
- Reduce transaction time via query/index fixes so connections return faster.
- Automate capacity planning based on concurrency and transaction duration.</p>
<p><strong>Questions I need answered:</strong>
- How many app instances connect to the pooler at peak?
- Are there deploy events that align with wait spikes (connection churn)?
- Are long-running queries holding connections open?</p>
<p><a id="finding-0006"></a></p>
<h4 id="high-systemd-service-appears-to-be-flapping-restart-loop">[High] systemd service appears to be flapping (restart loop)</h4>
<p><strong>Impact:</strong> Restart loops create intermittent downtime, amplify load (retry storms), and usually mask a real dependency issue (DB, DNS, config, or secrets). They also consume CPU and can trigger cascading failures.</p>
<p><strong>Confidence:</strong> High</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-b723445da9">fixtures/linux/systemctl_status.txt:L8-L8 (Restart counter is 27)</a></p>
<p><strong>Fix now:</strong> Pull recent logs and verify dependencies; add backoff while fixing root cause</p>
<pre><code class="language-bash">sudo journalctl -u api.service --since '2 hours ago' | tail -200
sudo systemctl show api.service -p Restart -p RestartUSec -p StartLimitBurst -p StartLimitIntervalUSec
sudo systemctl status api.service
</code></pre>
<p><strong>7-day plan:</strong>
- Identify the failing dependency (DB connectivity, DNS, secrets, config) and fix root cause.
- Add health checks and a reasonable restart policy (backoff + limits) to avoid retry storms.
- Add alerting on restart rate and error budget burn.</p>
<p><strong>30-day plan:</strong>
- Add graceful degradation (circuit breaker/backoff) in the app for dependency failures.
- Add dependency SLOs (DB latency, DNS) and correlate with deploy events.
- Standardize systemd unit templates and logging across services.</p>
<p><strong>Questions I need answered:</strong>
- Is this happening constantly or only during deploy windows?
- What database/network path does the service use (VPC, SG, local socket)?
- Do you have an incident timeline for when this started?</p>
<p><a id="finding-0007"></a></p>
<h4 id="medium-autovacuum-pressure-likely-on-publicorders-publicevents-publicsessions-with-high-dead-tuple-ratios">[Medium] Autovacuum pressure likely on (public.orders, public.events, public.sessions) with high dead tuple ratios</h4>
<p><strong>Impact:</strong> High dead tuples increase bloat and slow queries (more pages to scan, worse cache locality). If vacuum can't keep up, performance degrades and storage costs rise.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-14ed22c00e">fixtures/postgres/pg_stat_user_tables.csv (Tables with high n_dead_tup relative to n_live_tup)</a></p>
<p><strong>Fix now:</strong> Inspect worst tables and tune vacuum/analyze thresholds where needed</p>
<pre><code class="language-bash">psql -c &quot;SELECT relname, n_live_tup, n_dead_tup, last_autovacuum FROM pg_stat_user_tables ORDER BY n_dead_tup DESC LIMIT 20;&quot;
# Consider per-table tuning on hot churn tables:
# autovacuum_vacuum_scale_factor, autovacuum_vacuum_threshold, # autovacuum_analyze_scale_factor, autovacuum_analyze_threshold
# Also check for long-running transactions preventing cleanup.
</code></pre>
<p><strong>7-day plan:</strong>
- Identify top bloat contributors and confirm vacuum is running as expected.
- Adjust autovac settings for the highest-churn tables (sessions/events/orders).
- Add alerting for dead tuple ratio and vacuum lag.</p>
<p><strong>30-day plan:</strong>
- Schedule periodic bloat checks and reindex strategy where appropriate.
- Review retention policies (e.g., session cleanup) to reduce churn.
- Add runbooks for vacuum/reindex and long-transaction mitigation.</p>
<p><strong>Questions I need answered:</strong>
- Any long-running transactions or idle-in-transaction sessions during peaks?
- Are you using managed defaults (RDS/Aurora) or custom autovac settings?
- Do you have strict maintenance window constraints?</p>
<p><a id="finding-0008"></a></p>
<h4 id="medium-disk-usage-is-high-87-on-at-least-one-filesystem">[Medium] Disk usage is high (87%) on at least one filesystem</h4>
<p><strong>Impact:</strong> High disk usage is a common outage trigger (writes fail, services crash, databases stall). It also hides other problems (logs grow until the host falls over).</p>
<p><strong>Confidence:</strong> High</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-981e1d7f5a">fixtures/linux/df_h.txt:L2-L2 (Filesystem above threshold: /dev/nvme0n1p2  100G   87G   13G  87% /)</a></p>
<p><strong>Fix now:</strong> Identify top disk consumers and cap runaway logs safely</p>
<pre><code class="language-bash">sudo du -xh /var/log | sort -h | tail -50
sudo journalctl --disk-usage
sudo sed -i 's/^#SystemMaxUse=.*/SystemMaxUse=1G/' /etc/systemd/journald.conf || true
sudo systemctl restart systemd-journald || true
</code></pre>
<p><strong>7-day plan:</strong>
- Confirm alerting on disk % and inode usage (thresholds + paging policy).
- Implement log rotation policy for app logs and set journald caps.
- Add a runbook: safe cleanup + where growth typically comes from.</p>
<p><strong>30-day plan:</strong>
- Add SLO-driven alerting and capacity planning (trend disk growth).
- Standardize log retention per environment (dev/stage/prod).
- Automate checks in CI or daily cron to catch regressions.</p>
<p><strong>Questions I need answered:</strong>
- Is this host stateful (DB) or stateless (app)? Cleanup approach differs.
- Any known log bursts (deploys, retries, noisy errors) causing growth?
- What is your on-call policy for disk alerts (page vs ticket)?</p>
<h3 id="performance">Performance</h3>
<p><a id="finding-0003"></a></p>
<h4 id="high-high-sequential-scan-activity-on-large-tables-publicorders-publicevents">[High] High sequential scan activity on large tables (public.orders, public.events)</h4>
<p><strong>Impact:</strong> Repeated sequential scans on large tables inflate latency and CPU, especially under concurrency. This is a common root cause of 'DB is slow' incidents.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-909b017bb4">fixtures/postgres/pg_stat_user_tables.csv (Tables with high reltuples and high seq_scan)</a></p>
<p><strong>Fix now:</strong> Identify query patterns causing seq_scans and add targeted indexes</p>
<pre><code class="language-bash"># Map top seq_scans to query patterns (pg_stat_statements + logs)
# Run EXPLAIN (ANALYZE, BUFFERS) to confirm scan type and cost
# Add the smallest viable index to support the common filter/order
psql -c &quot;SELECT relname, seq_scan, idx_scan, n_live_tup, n_dead_tup FROM pg_stat_user_tables ORDER BY seq_scan DESC LIMIT 20;&quot;
</code></pre>
<p><strong>7-day plan:</strong>
- Map top seq_scanned tables to specific endpoints/jobs.
- Implement 1-2 high-ROI fixes (index or query rewrite) and measure p95 before/after.
- Ensure stats are current (ANALYZE) for affected tables.</p>
<p><strong>30-day plan:</strong>
- Add performance dashboards/alerts (DB CPU, buffer hit rate, slow query spikes).
- Review ORM/query patterns (wide SELECT *, missing filters) driving scans.
- Consider partitioning for large time-series tables if growth continues.</p>
<p><strong>Questions I need answered:</strong>
- Are these tables expected to be scan-heavy (analytics), or OLTP hot paths?
- Do you have read replicas or a separate analytics store?
- Any existing indexes that are unused or misaligned with query patterns?</p>
<p><a id="finding-0004"></a></p>
<h4 id="high-postgres-shows-heavy-time-spent-in-a-small-set-of-queries-pg_stat_statements">[High] Postgres shows heavy time spent in a small set of queries (pg_stat_statements)</h4>
<p><strong>Impact:</strong> A handful of queries often dominate database load. Improving them typically reduces p95 latency, stabilizes CPU, and lowers infra cost by delaying scale-up.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Medium / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-473008823f">fixtures/postgres/pg_stat_statements.csv:L1-L4 (Top queries by total_time_ms (sample))</a></p>
<p><strong>Fix now:</strong> Validate query plans and implement the highest-impact index/query changes</p>
<pre><code class="language-bash"># For each top query, run EXPLAIN (ANALYZE, BUFFERS) in a safe environment
# Confirm indexes with \d+ &lt;table&gt; and actual query patterns (params, ordering)
# Candidate index statements (validate with EXPLAIN + production constraints):
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email ON public.users (email);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_orders_created_at ON public.orders (created_at DESC);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sessions_token ON public.sessions (session_token);
</code></pre>
<p><strong>7-day plan:</strong>
- Confirm pg_stat_statements is enabled and capturing representative traffic.
- Run EXPLAIN (ANALYZE, BUFFERS) for top queries and identify scans/sorts/hot joins.
- Implement 1-2 highest-ROI fixes (index or query rewrite) with safe rollout.</p>
<p><strong>30-day plan:</strong>
- Add performance regression tests (key endpoints) and track DB p95 + CPU.
- Introduce SLO/alerts for slow query spikes and lock contention.
- Consider connection pooling tuning to reduce per-query overhead.</p>
<p><strong>Questions I need answered:</strong>
- Are these queries representative of peak traffic (same workload + time window)?
- Any hard constraints on index build time / lock tolerance?
- Is read/write split or partitioning on the roadmap?</p>
<h3 id="cost">Cost</h3>
<p><a id="finding-0001"></a></p>
<h4 id="medium-possible-overprovisioning-signal-m52xlarge-at-low-p95-utilization">[Medium] Possible overprovisioning signal: m5.2xlarge at low p95 utilization</h4>
<p><strong>Impact:</strong> If sustained utilization is low, you may be paying for capacity you don't need. Rightsizing can reduce spend without reducing reliability (when validated carefully).</p>
<p><strong>Confidence:</strong> Low</p>
<p><strong>Effort / Blast radius:</strong> Medium / High</p>
<p><strong>Evidence:</strong>
- <a href="#ev-87d6e8aded">fixtures/cost/utilization_summary.json (instance=i-0123456789abcdef0, cpu_p95=12.4%, mem_p95=28.1% over 30 days)</a></p>
<p><strong>Fix now:</strong> Create a rightsizing candidate and validate against peak/burst patterns</p>
<pre><code class="language-bash"># Validate with a larger window and include disk + network + burst behavior
# If safe, test downsize one step in a canary environment first
aws cloudwatch get-metric-statistics ...  # (example: CPUUtilization p95/p99)
</code></pre>
<p><strong>Validation / success / rollback:</strong>
- Validate safely: Validate with 30–90d metrics and test one-step downsize on a canary; ensure p95/p99 latency and error rate do not regress.
- Success metric: Monthly spend reduced without SLO regression (p95 latency, error rate, saturation).
- Rollback: Scale back to prior instance type/size immediately; revert autoscaling/schedule changes if applied.</p>
<p><strong>7-day plan:</strong>
- Pull 30-90d utilization including peak events and deploy windows.
- Identify a safe canary target for downsize and test rollback.
- Estimate savings and risk; execute one change with monitoring.</p>
<p><strong>30-day plan:</strong>
- Adopt scheduled scaling or autoscaling where appropriate.
- Track unit cost per request/job and alert on regressions.
- Automate monthly cost posture checks and recommendations.</p>
<p><strong>Questions I need answered:</strong>
- Are there known weekly/monthly peaks not represented in this sample?
- Any CPU credit/burstable instances involved?
- What is your rollback plan if latency increases after downsize?</p>
<p><a id="finding-0002"></a></p>
<h4 id="low-ebs-gp2-volumes-detected-consider-gp3-for-costperformance-control">[Low] EBS gp2 volumes detected; consider gp3 for cost/performance control</h4>
<p><strong>Impact:</strong> gp3 often provides better baseline performance and more predictable tuning. Switching from gp2 to gp3 can reduce cost and decouple size from performance.</p>
<p><strong>Confidence:</strong> Medium</p>
<p><strong>Effort / Blast radius:</strong> Low / Medium</p>
<p><strong>Evidence:</strong>
- <a href="#ev-4b5c109bc2">fixtures/cost/ebs_volumes.csv (gp2 volumes: vol-0aaa111)</a></p>
<p><strong>Fix now:</strong> Evaluate gp3 migration plan (low-risk, validate per workload)</p>
<pre><code class="language-bash"># In AWS: modify volume type to gp3 and set IOPS/throughput as needed
# Validate latency/IOPS requirements before and after
aws ec2 modify-volume --volume-id &lt;vol-id&gt; --volume-type gp3 --iops 3000 --throughput 125
</code></pre>
<p><strong>Validation / success / rollback:</strong>
- Validate safely: Migrate a non-critical volume first; compare I/O latency and throughput before/after under normal and peak load.
- Success metric: Lower storage cost and/or improved baseline IOPS/throughput with no latency regressions.
- Rollback: Switch volume type back (or increase gp3 IOPS/throughput) if latency regresses.</p>
<p><strong>7-day plan:</strong>
- Inventory gp2 volumes and identify those safe to migrate first.
- Migrate a non-critical volume and confirm workload metrics.
- Roll out remaining migrations with a change window + monitoring.</p>
<p><strong>30-day plan:</strong>
- Standardize volume types/policies in IaC (default to gp3).
- Add cost posture checks for storage, snapshots, and idle resources.</p>
<p><strong>Questions I need answered:</strong>
- Are there workloads with unusually high IOPS/throughput requirements?
- Do you have maintenance windows for volume modifications?</p>
<h2 id="raw-evidence">Raw evidence</h2>
<p>Evidence links above jump here. Snippets are extracted from the fixture bundle used to generate this report.</p>
<p><a id="ev-14ed22c00e"></a></p>
<h3 id="fixturespostgrespg_stat_user_tablescsv-tables-with-high-n_dead_tup-relative-to-n_live_tup">fixtures/postgres/pg_stat_user_tables.csv (Tables with high n_dead_tup relative to n_live_tup)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: schemaname,relname,seq_scan,seq_tup_read,idx_scan,n_tup_ins,n_tup_upd,n_tup_del,n_live_tup,n_dead_tup,last_vacuum,last_autovacuum,last_analyze,last_autoanalyze,reltuples
    2: public,orders,18250,92384012,1200,250000,12000,800,2150000,420000,,2025-12-29 03:10:00,2025-12-30 04:10:00,2025-12-31 04:10:00,2300000
    3: public,events,9050,55120000,500,1100000,5000,1000,9100000,1250000,,2025-12-15 02:00:00,2025-12-20 02:00:00,2025-12-20 02:00:00,10300000
    4: public,users,120,8200,55000,5000,2000,10,84000,1200,2025-12-28 01:00:00,2025-12-31 01:00:00,2025-12-31 01:05:00,2025-12-31 01:05:00,84000
    5: public,sessions,240,56000,2200,800000,900000,200000,1200000,950000,,2025-12-10 01:00:00,2025-12-10 01:05:00,2025-12-10 01:05:00,1250000
</code></pre>


</details>

<p><a id="ev-23fd6b3517"></a></p>
<h3 id="fixturesedgetls_scantxt-hsts-marked-missing-in-scan-summary">fixtures/edge/tls_scan.txt (HSTS marked missing in scan summary)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: # Fake TLS scan summary
    2: TLSv1.0: enabled
    3: TLSv1.1: enabled
    4: TLSv1.2: enabled
    5: TLSv1.3: enabled
    6: HSTS: missing
    7: Cipher suites: includes some legacy CBC suites
</code></pre>


</details>

<p><a id="ev-38959d52f8"></a></p>
<h3 id="fixtureslinuxss_lntptxtl5-l5-bound-to-00005432">fixtures/linux/ss_lntp.txt:L5-L5 (Bound to 0.0.0.0:5432)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    5: LISTEN 0      244    0.0.0.0:5432         0.0.0.0:*          users:((&quot;postgres&quot;,pid=1201,fd=7))
</code></pre>


</details>

<p><a id="ev-473008823f"></a></p>
<h3 id="fixturespostgrespg_stat_statementscsvl1-l4-top-queries-by-total_time_ms-sample">fixtures/postgres/pg_stat_statements.csv:L1-L4 (Top queries by total_time_ms (sample))</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: queryid,calls,total_time_ms,mean_time_ms,rows,query
    2: 1001,8421,912345.5,108.3,8421,&quot;SELECT id,email,last_login FROM users WHERE email = $1&quot;
    3: 1002,421,602110.2,1430.4,421,&quot;SELECT * FROM orders WHERE created_at &gt;= $1 AND created_at &lt; $2 ORDER BY created_at DESC LIMIT 200&quot;
    4: 1003,11021,401221.9,36.4,11021,&quot;UPDATE sessions SET expires_at = $1 WHERE session_token = $2&quot;
</code></pre>


</details>

<p><a id="ev-4b5c109bc2"></a></p>
<h3 id="fixturescostebs_volumescsv-gp2-volumes-vol-0aaa111">fixtures/cost/ebs_volumes.csv (gp2 volumes: vol-0aaa111)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: volume_id,type,size_gb,iops,throughput_mbps,attached_instance_id
    2: vol-0aaa111,gp2,500,1500,,i-0123456789abcdef0
    3: vol-0bbb222,gp3,200,3000,125,i-0123456789abcdef0
</code></pre>


</details>

<p><a id="ev-5ee8609c2c"></a></p>
<h3 id="fixturespostgrespg_pool_statsjson-clients48605000-waiting322-avg_wait_ms1850">fixtures/postgres/pg_pool_stats.json (clients=4860/5000, waiting=322, avg_wait_ms=185.0)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: {
    2:   &quot;pooler&quot;: &quot;pgbouncer&quot;,
    3:   &quot;db&quot;: &quot;app&quot;,
    4:   &quot;max_client_conn&quot;: 5000,
    5:   &quot;default_pool_size&quot;: 50,
    6:   &quot;current_clients&quot;: 4860,
    7:   &quot;current_waiting&quot;: 322,
    8:   &quot;avg_wait_ms&quot;: 185,
    9:   &quot;peak_wait_ms&quot;: 2400,
   10:   &quot;notes&quot;: &quot;Spikes observed during deploy window&quot;
   11: }
</code></pre>


</details>

<p><a id="ev-87d6e8aded"></a></p>
<h3 id="fixturescostutilization_summaryjson-instancei-0123456789abcdef0-cpu_p95124-mem_p95281-over-30-days">fixtures/cost/utilization_summary.json (instance=i-0123456789abcdef0, cpu_p95=12.4%, mem_p95=28.1% over 30 days)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: {
    2:   &quot;instance_id&quot;: &quot;i-0123456789abcdef0&quot;,
    3:   &quot;instance_type&quot;: &quot;m5.2xlarge&quot;,
    4:   &quot;cpu_p95_percent&quot;: 12.4,
    5:   &quot;cpu_p50_percent&quot;: 4.8,
    6:   &quot;memory_p95_percent&quot;: 28.1,
    7:   &quot;network_p95_mbps&quot;: 42.0,
    8:   &quot;period_days&quot;: 30,
    9:   &quot;notes&quot;: &quot;Workload steady; no batch peaks detected&quot;
   10: }
</code></pre>


</details>

<p><a id="ev-8eba99682e"></a></p>
<h3 id="fixturesedgetls_scantxt-tlsv10tlsv11-enabled-in-scan-summary">fixtures/edge/tls_scan.txt (TLSv1.0/TLSv1.1 enabled in scan summary)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: # Fake TLS scan summary
    2: TLSv1.0: enabled
    3: TLSv1.1: enabled
    4: TLSv1.2: enabled
    5: TLSv1.3: enabled
    6: HSTS: missing
    7: Cipher suites: includes some legacy CBC suites
</code></pre>


</details>

<p><a id="ev-909b017bb4"></a></p>
<h3 id="fixturespostgrespg_stat_user_tablescsv-tables-with-high-reltuples-and-high-seq_scan">fixtures/postgres/pg_stat_user_tables.csv (Tables with high reltuples and high seq_scan)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    1: schemaname,relname,seq_scan,seq_tup_read,idx_scan,n_tup_ins,n_tup_upd,n_tup_del,n_live_tup,n_dead_tup,last_vacuum,last_autovacuum,last_analyze,last_autoanalyze,reltuples
    2: public,orders,18250,92384012,1200,250000,12000,800,2150000,420000,,2025-12-29 03:10:00,2025-12-30 04:10:00,2025-12-31 04:10:00,2300000
    3: public,events,9050,55120000,500,1100000,5000,1000,9100000,1250000,,2025-12-15 02:00:00,2025-12-20 02:00:00,2025-12-20 02:00:00,10300000
    4: public,users,120,8200,55000,5000,2000,10,84000,1200,2025-12-28 01:00:00,2025-12-31 01:00:00,2025-12-31 01:05:00,2025-12-31 01:05:00,84000
    5: public,sessions,240,56000,2200,800000,900000,200000,1200000,950000,,2025-12-10 01:00:00,2025-12-10 01:05:00,2025-12-10 01:05:00,1250000
</code></pre>


</details>

<p><a id="ev-981e1d7f5a"></a></p>
<h3 id="fixtureslinuxdf_htxtl2-l2-filesystem-above-threshold-devnvme0n1p2-100g-87g-13g-87">fixtures/linux/df_h.txt:L2-L2 (Filesystem above threshold: /dev/nvme0n1p2  100G   87G   13G  87% /)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    2: /dev/nvme0n1p2  100G   87G   13G  87% /
</code></pre>


</details>

<p><a id="ev-b723445da9"></a></p>
<h3 id="fixtureslinuxsystemctl_statustxtl8-l8-restart-counter-is-27">fixtures/linux/systemctl_status.txt:L8-L8 (Restart counter is 27)</h3>
<details>
<summary>Show snippet</summary>


<pre><code class="language-text">    8: Jan 05 07:41:22 host systemd[1]: api.service: Scheduled restart job, restart counter is at 27.
</code></pre>


</details>
  </div>
</body>
</html>
